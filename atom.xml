<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>焕小妹</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-09-14T03:21:22.699Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>焕焕</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>关于转载</title>
    <link href="http://yoursite.com/2019/09/12/copyright-reprinted/"/>
    <id>http://yoursite.com/2019/09/12/copyright-reprinted/</id>
    <published>2019-09-12T13:55:27.000Z</published>
    <updated>2019-09-14T03:21:22.699Z</updated>
    
    <content type="html"><![CDATA[<h2 id="我希望的转载方式-✅"><a href="#我希望的转载方式-✅" class="headerlink" title="我希望的转载方式 ✅"></a>我希望的转载方式 ✅</h2><p><strong>分享链接转载，指向我的博文。</strong> 我觉的这是最好的转载方式，互赢。</p><h2 id="使用别人的原创"><a href="#使用别人的原创" class="headerlink" title="使用别人的原创"></a>使用别人的原创</h2><p>换位思考，以后我使用别人的图片和摘选的内容时，我会带上来源地址和作者署名，尊重原创。💪</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;我希望的转载方式-✅&quot;&gt;&lt;a href=&quot;#我希望的转载方式-✅&quot; class=&quot;headerlink&quot; title=&quot;我希望的转载方式 ✅&quot;&gt;&lt;/a&gt;我希望的转载方式 ✅&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;分享链接转载，指向我的博文。&lt;/strong&gt; 我觉的这是最
      
    
    </summary>
    
    
      <category term="版权声明" scheme="http://yoursite.com/categories/%E7%89%88%E6%9D%83%E5%A3%B0%E6%98%8E/"/>
    
    
      <category term="软件安装" scheme="http://yoursite.com/tags/%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2019/09/12/mac%E4%B8%8A%E4%BD%BF%E7%94%A8%E6%8D%B7%E5%B8%8C%E7%A7%BB%E5%8A%A8%E7%A1%AC%E7%9B%98/"/>
    <id>http://yoursite.com/2019/09/12/mac上使用捷希移动硬盘/</id>
    <published>2019-09-12T01:45:10.615Z</published>
    <updated>2019-09-12T02:19:39.402Z</updated>
    
    <content type="html"><![CDATA[<p>内心OS：被困扰18个小时的question，终于解决，只因思路不清晰，太蠢！–20190912</p><h2 id="设备描述"><a href="#设备描述" class="headerlink" title="设备描述"></a>设备描述</h2><ul><li><p>MacBcook Pro -2018 version：10.14.3 </p></li><li><p>希捷硬盘购买信息：希捷(Seagate) 1TB USB3.0 移动硬盘 睿品新版铭 兼容Mac</p><p>## </p></li></ul><h2 id="问题解决步骤"><a href="#问题解决步骤" class="headerlink" title="问题解决步骤"></a>问题解决步骤</h2><ol><li><p><a href="https://www.seagate.com/cn/zh/support/downloads/" target="_blank" rel="noopener">捷希官网</a>下载 Paragon 驱动程序（注意mac版本）</p></li><li><p>下载完成后双击软件，按照提示步骤完成安装</p></li><li><p>重启Mac</p></li><li><p>捷希硬盘插入mac</p></li><li><p>打开 <strong>NTFS for Mac</strong> 软件，发现移动硬盘已显示，如下图</p><p><img src alt="image1"><span class="img-alt">image1</span></p></li><li><p>点击 右侧 磁盘名称（）即可正常访问磁盘</p></li></ol><h2 id="问题解答"><a href="#问题解答" class="headerlink" title="问题解答"></a>问题解答</h2><ol><li>如何修改硬盘名称？<ul><li>参见步骤6</li></ul></li></ol><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><p>1.<a href="https://www.bilibili.com/video/av8611848/" target="_blank" rel="noopener">如何在Mac系统下正确使用硬盘-视频讲解</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;内心OS：被困扰18个小时的question，终于解决，只因思路不清晰，太蠢！–20190912&lt;/p&gt;
&lt;h2 id=&quot;设备描述&quot;&gt;&lt;a href=&quot;#设备描述&quot; class=&quot;headerlink&quot; title=&quot;设备描述&quot;&gt;&lt;/a&gt;设备描述&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Hexo+github 从windows到mac的迁移（超详细步骤及遇到问题解决）（特适合小白）</title>
    <link href="http://yoursite.com/2019/09/10/Hexo+github%20%E4%BB%8Ewindows%E5%88%B0mac%E7%9A%84%E8%BF%81%E7%A7%BB%EF%BC%88%E8%B6%85%E8%AF%A6%E7%BB%86%E6%AD%A5%E9%AA%A4%E5%8F%8A%E9%81%87%E5%88%B0%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%EF%BC%89/"/>
    <id>http://yoursite.com/2019/09/10/Hexo+github 从windows到mac的迁移（超详细步骤及遇到问题解决）/</id>
    <published>2019-09-10T13:55:27.000Z</published>
    <updated>2019-09-14T03:35:20.107Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>2019的年5月在惠普笔记本搭建了个hexo+github版人博客就；<br>2019的6月刚入手了一个mac，于是乎各种东西都得由windows转移到mac;<br>hexo的搬家路：windows - &gt; MacBook Pro </p><h2 id="总体思路"><a href="#总体思路" class="headerlink" title="总体思路"></a>总体思路</h2><p>理清思路真不难，过程要细心、耐心，大概分为以下三个模块：</p><ul><li>首先在mac电脑上安装好<code>hexo</code>，并初始化根目录;</li><li>然后生成新的<code>SSH key</code>，并将其添加到<code>github</code>上;</li><li>将旧电脑中的三个文件，直接粘贴覆盖在新电脑对应的目录。</li></ul><h2 id="言归正传"><a href="#言归正传" class="headerlink" title="言归正传"></a>言归正传</h2><h2 id="1-安装hexo前奏"><a href="#1-安装hexo前奏" class="headerlink" title="1. 安装hexo前奏"></a>1. 安装hexo前奏</h2><h2 id="2-安装Hexo"><a href="#2-安装Hexo" class="headerlink" title="2. 安装Hexo"></a>2. 安装Hexo</h2><h2 id="3-mac生成SSH"><a href="#3-mac生成SSH" class="headerlink" title="3.mac生成SSH"></a>3.mac生成SSH</h2><h2 id="4-替换-config-yml-、thems、source-文件"><a href="#4-替换-config-yml-、thems、source-文件" class="headerlink" title="4. 替换_config.yml 、thems、source 文件"></a>4. 替换_config.yml 、thems、source 文件</h2><h2 id="更换主题专题"><a href="#更换主题专题" class="headerlink" title="更换主题专题"></a>更换主题专题</h2><h3 id="内心OS："><a href="#内心OS：" class="headerlink" title="内心OS："></a>内心OS：</h3><p><strong>对现有主题不满意, 缺失某种功能（eg. 评论、时间轴）</strong></p><p><strong>我是代码小白，别人的博客太厉害！</strong> </p><p><strong>别当心，其实都是纸老虎，这里啥都有～～</strong></p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ol><li><a href="[https://gjincai.github.io/2017/01/13/hexo-%E4%BB%8E-windows-%E8%BD%AC%E7%A7%BB%E8%87%B3-Mac/](https://gjincai.github.io/2017/01/13/hexo-从-windows-转移至-Mac/)">hexo从windows转移至Mac</a></li><li><a href="https://blog.csdn.net/qq_39153421/article/details/89362432" target="_blank" rel="noopener">解决hexo -d 报错问题</a></li><li><a href="[https://github.com/Youthink/hexo-themes-yearn/wiki/%E4%B8%BB%E9%A2%98%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B-%F0%9F%93%96](https://github.com/Youthink/hexo-themes-yearn/wiki/主题使用教程-📖)">hexo搭建模版参照教程</a></li><li><a href="https://github.com/adrai/flowchart.js" target="_blank" rel="noopener">用<strong>flowchart.js</strong>画流程图</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;2019的年5月在惠普笔记本搭建了个hexo+github版人博客就；&lt;br&gt;2019的6月刚入手了一个mac，于是乎各种东西都得由wind
      
    
    </summary>
    
    
      <category term="技术篇" scheme="http://yoursite.com/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="软件安装" scheme="http://yoursite.com/tags/%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85/"/>
    
  </entry>
  
  <entry>
    <title>Typora（一款比Markdown更好用的软件编辑器）使用方法简介.md</title>
    <link href="http://yoursite.com/2019/09/10/Typora%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%E7%AE%80%E4%BB%8B/"/>
    <id>http://yoursite.com/2019/09/10/Typora使用方法简介/</id>
    <published>2019-09-10T13:55:27.000Z</published>
    <updated>2019-09-14T03:14:16.941Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Typora编辑器使用方法–参考博客链接"><a href="#Typora编辑器使用方法–参考博客链接" class="headerlink" title="Typora编辑器使用方法–参考博客链接"></a><a href="https://blog.csdn.net/wirelessqa/article/details/70432631" target="_blank" rel="noopener">Typora编辑器使用方法–参考博客链接</a></h3><h2 id="格式"><a href="#格式" class="headerlink" title="格式"></a>格式</h2><p>  <strong>粗体</strong>、<em>斜体</em>、==高亮==、<del>删除线</del>、<u>下划线</u>、我是^上标^、我是 ~下标 ~、<a href="http://www.baidu.com" target="_blank" rel="noopener">超链接</a></p><p>  <img src="https://img3.doubanio.com/view/movie_poster_cover/lpst/public/p2411953504.jpg" alt="示例图片"><span class="img-alt">示例图片</span></p><h2 id="无序列表"><a href="#无序列表" class="headerlink" title="无序列表"></a>无序列表</h2><ul><li><p>无序列表1</p></li><li><p>无序列表2</p><h2 id="有序列表"><a href="#有序列表" class="headerlink" title="有序列表"></a>有序列表</h2></li></ul><ol><li><p>有序列表</p></li><li><p>有序列表</p><h2 id="任务列表"><a href="#任务列表" class="headerlink" title="任务列表"></a>任务列表</h2></li></ol><ul><li><input disabled type="checkbox"> <p>看电影</p></li><li><input disabled type="checkbox"> <p>听音乐</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Typora编辑器使用方法–参考博客链接&quot;&gt;&lt;a href=&quot;#Typora编辑器使用方法–参考博客链接&quot; class=&quot;headerlink&quot; title=&quot;Typora编辑器使用方法–参考博客链接&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://blog.csdn
      
    
    </summary>
    
    
      <category term="技术篇" scheme="http://yoursite.com/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="软件安装" scheme="http://yoursite.com/tags/%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85/"/>
    
  </entry>
  
  <entry>
    <title>Skiil about ubantu</title>
    <link href="http://yoursite.com/2019/09/10/Skiil%20about%20ubantu%20/"/>
    <id>http://yoursite.com/2019/09/10/Skiil about ubantu /</id>
    <published>2019-09-10T13:55:27.000Z</published>
    <updated>2019-09-13T10:14:43.230Z</updated>
    
    <content type="html"><![CDATA[<p>## </p><h2 id="ubantu下常用的一些命令及快捷键"><a href="#ubantu下常用的一些命令及快捷键" class="headerlink" title="ubantu下常用的一些命令及快捷键"></a>ubantu下常用的一些命令及快捷键</h2><ol><li>cd..      后退一步</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;## &lt;/p&gt;
&lt;h2 id=&quot;ubantu下常用的一些命令及快捷键&quot;&gt;&lt;a href=&quot;#ubantu下常用的一些命令及快捷键&quot; class=&quot;headerlink&quot; title=&quot;ubantu下常用的一些命令及快捷键&quot;&gt;&lt;/a&gt;ubantu下常用的一些命令及快捷键&lt;/h
      
    
    </summary>
    
    
      <category term="技术篇" scheme="http://yoursite.com/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="快捷键使用" scheme="http://yoursite.com/tags/%E5%BF%AB%E6%8D%B7%E9%94%AE%E4%BD%BF%E7%94%A8/"/>
    
  </entry>
  
  <entry>
    <title>softwaerVersion</title>
    <link href="http://yoursite.com/2019/05/11/softwaerVersion/"/>
    <id>http://yoursite.com/2019/05/11/softwaerVersion/</id>
    <published>2019-05-11T03:57:15.000Z</published>
    <updated>2019-05-11T04:54:50.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Ubantu-系统"><a href="#Ubantu-系统" class="headerlink" title="Ubantu 系统"></a>Ubantu 系统</h2><p>本文为在为Ubantu系统安装tensorflow时，因为已有一些软件的安装，所以需要查看确认下</p><p><strong>首先 快捷键（Ctrl+Alt+t）打开终端</strong></p><h2 id="CUDA"><a href="#CUDA" class="headerlink" title="CUDA"></a>CUDA</h2><blockquote><p>nvcc -V  </p></blockquote><p>！！ 注意 V 是大写</p><h2 id="CUDNN"><a href="#CUDNN" class="headerlink" title="CUDNN"></a>CUDNN</h2><blockquote><p>cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR  </p></blockquote><h2 id="GCC"><a href="#GCC" class="headerlink" title="GCC"></a>GCC</h2><p>gcc -v</p><p>！！ 注意 v 是小写</p><h2 id="Anaconda"><a href="#Anaconda" class="headerlink" title="Anaconda"></a>Anaconda</h2><blockquote><p>conda -V</p></blockquote><p>！！ 注意 V 是大写  </p><h2 id="Ubantu"><a href="#Ubantu" class="headerlink" title="Ubantu"></a>Ubantu</h2><blockquote><ol><li>cat /proc/version  </li><li>uname -a  </li><li>lsb_release -a  </li></ol></blockquote><h2 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h2><p>1.<a href="https://medium.com/@changrongko/nv-how-to-check-cuda-and-cudnn-version-e05aa21daf6c" target="_blank" rel="noopener">查看 CUDA cudnn 版本</a>  </p><ol start="2"><li><a href="https://jingyan.baidu.com/article/2c8c281d87c9890009252a41.html" target="_blank" rel="noopener">ubuntu查看gcc的版本</a>   </li><li><a href="https://blog.csdn.net/wyx100/article/details/79453941" target="_blank" rel="noopener">查看Anaconda版本、Anaconda和python版本对应关系和快速下载</a>     </li><li><a href="https://zhidao.baidu.com/question/154707237.html?fr=iks&word=%C8%E7%BA%CE%D3%C3%C3%FC%C1%EE%B2%E9%D1%AF+Ubantu+%B0%E6%B1%BE%BA%C5&ie=gbk" target="_blank" rel="noopener">如何用命令查询 Ubantu 版本号</a>  </li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Ubantu-系统&quot;&gt;&lt;a href=&quot;#Ubantu-系统&quot; class=&quot;headerlink&quot; title=&quot;Ubantu 系统&quot;&gt;&lt;/a&gt;Ubantu 系统&lt;/h2&gt;&lt;p&gt;本文为在为Ubantu系统安装tensorflow时，因为已有一些软件的安装，所以需
      
    
    </summary>
    
    
      <category term="software" scheme="http://yoursite.com/categories/software/"/>
    
    
      <category term="Version" scheme="http://yoursite.com/tags/Version/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统评价--NDCG方法概述</title>
    <link href="http://yoursite.com/2019/05/02/RSC-NDCG/"/>
    <id>http://yoursite.com/2019/05/02/RSC-NDCG/</id>
    <published>2019-05-02T07:48:25.000Z</published>
    <updated>2019-05-02T09:14:46.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="NDCG方法概述"><a href="#NDCG方法概述" class="headerlink" title="NDCG方法概述"></a>NDCG方法概述</h2><p><strong>NDCG(Normalized Discounted Cumulative Gain)</strong>：计算相对复杂。对于排在结位置n处的NDCG的计算公式如下图所示：<br><img src="https://github.com/HuanwenW/MyPostImag/blob/master/190502-Res/NDCG.png?raw=true" alt="NDCG公式"><span class="img-alt">NDCG公式</span>  </p><p>&ensp;&ensp;&ensp;一个推荐系统返回一些项并形成一个列表，我们想要计算这个列表有多好。每一项都有一个相关的评分值，通常这些评分值是一个非负数。这就是<strong>gain（增益）</strong>。此外，对于这些没有用户反馈的项，我们通常设置其增益为0。 </p><p><strong>例如：假设有两个主题，主题1有4个相关网页，主题2有5个相关网页。</strong>  </p><p>(1) 相关度分成从0到r的r+1的等级(r可设定)。当取r=5时，等级设定如下图所示：</p><p><img src="https://github.com/HuanwenW/MyPostImag/blob/master/190502-Res/NDCG-gain.png?raw=true" alt="NDCG-增益"><span class="img-alt">NDCG-增益</span><br>(应该还有r=1那一级，原文档有误，不过这里不影响理解)   </p><p>(2) 例如现在有一个query={abc}，返回下图左列的Ranked List(URL)，当假设用户的选择与排序结果无关(即每一级都等概率被选中)，则生成的累计增益值如下图最右列所示：<br><img src="https://github.com/HuanwenW/MyPostImag/blob/master/190502-Res/NDCG-CumulativeGain.png?raw=true" alt="NDCG-累计增益"><span class="img-alt">NDCG-累计增益</span><br>&ensp;&ensp;&ensp;我们把这些分数相加，也就是<strong>Cumulative Gain（累积增益）</strong>。我们更愿意看那些位于列表前面的最相关的项，因此，在把这些分数相加之前，我们将每项除以一个递增的数（通常是该项位置的对数值），也就是折损值，并得到DCG。 </p><p>(3) 考虑到一般情况下用户会优先点选排在前面的搜索结果，所以应该引入一个折算因子(discounting factor): log(2)/log(1+rank)。这时将获得DCG值(Discounted Cumulative Gain)如下如所示：<br><img src="https://github.com/HuanwenW/MyPostImag/blob/master/190502-Res/NDCG-gain.png?raw=true" alt="NDCG-折算因子"><span class="img-alt">NDCG-折算因子</span>    </p><p>(4) &ensp;&ensp;&ensp;在用户与用户之间，DCGs没有直接的可比性，所以我们要对它们进行归一化处理。即为了使不同等级上的搜索结果的得分值容易比较，需要将DCG值归一化的到NDCG值。操作如下图所示，首先计算理想返回结果List的DCG值：<br><img src="https://github.com/HuanwenW/MyPostImag/blob/master/190502-Res/NDCG-Normalized.png?raw=true" alt="NDCG-归一化"><span class="img-alt">NDCG-归一化</span>  </p><p>(5) 然后用DCG/MaxDCG就得到NDCG值，如下图所示：<br><img src="https://github.com/HuanwenW/MyPostImag/blob/master/190502-Res/NDCG-gain.png?raw=true" alt="NDCG-比值"><span class="img-alt">NDCG-比值</span>    </p><p>&ensp;&ensp;&ensp; 最糟糕的情况是，当使用非负相关评分时DCG为0。为了得到最好的，我们把测试集中所有的条目置放在理想的次序下，采取的是前K项并计算它们的DCG。然后将原DCG除以理想状态下的DCG并得到<strong>NDCG@K</strong>，它是一个0到1之间的数。<br>&ensp;&ensp;&ensp;你可能已经注意到，我们使用K表示推荐列表的长度。这个数由专业人员指定。你可以把它想像成是一个用户可能会注意到的多少个项的一个估计值，如10或50这些比较常见的值。</p><h2 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h2><ol><li><a href="https://blog.csdn.net/u010670689/article/details/73196054" target="_blank" rel="noopener"><strong>推荐系统评价：NDCG方法概述</strong></a>  </li><li><a href="https://blog.csdn.net/lujiandong1/article/details/77123805" target="_blank" rel="noopener">NDCG及其实现</a>  </li><li><a href="https://blog.csdn.net/weixin_38405636/article/details/80675312" target="_blank" rel="noopener">排序算法常用评价指标计算方式（AUC,MAP,NDCG,MRR）</a>  </li><li><a href="https://www.cnblogs.com/eyeszjwang/articles/2368087.html" target="_blank" rel="noopener"><strong>Learning to Rank for IR的评价指标—MAP,NDCG,MRR</strong></a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;NDCG方法概述&quot;&gt;&lt;a href=&quot;#NDCG方法概述&quot; class=&quot;headerlink&quot; title=&quot;NDCG方法概述&quot;&gt;&lt;/a&gt;NDCG方法概述&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;NDCG(Normalized Discounted Cumulative 
      
    
    </summary>
    
    
      <category term="Res" scheme="http://yoursite.com/categories/Res/"/>
    
    
      <category term="推荐系统" scheme="http://yoursite.com/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Welcome to NLP-2019-huanhuan-homework-11</title>
    <link href="http://yoursite.com/2019/04/27/NLP-RNN-11/"/>
    <id>http://yoursite.com/2019/04/27/NLP-RNN-11/</id>
    <published>2019-04-27T12:00:57.000Z</published>
    <updated>2019-04-27T12:44:06.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="内容涵盖"><a href="#内容涵盖" class="headerlink" title="内容涵盖"></a>内容涵盖</h2><ul><li>RNN的结构。循环神经网络的提出背景、优缺点。着重学习RNN的反向传播、RNN出现的问题（梯度问题、长期依赖问题）、BPTT算法。</li><li>双向RNN</li><li>LSTM、GRU的结构、提出背景、优缺点。</li><li>针对梯度消失（LSTM等其他门控RNN）、梯度爆炸（梯度截断）的解决方案。</li><li>Text-RNN的原理。</li><li>利用Text-RNN模型来进行文本分类。<h2 id="RNN-基础"><a href="#RNN-基础" class="headerlink" title="RNN 基础"></a>RNN 基础</h2>&ensp;&ensp;&ensp;RNN（Recurrent Neural Network）是一类用于处理序列数据的神经网络。首先我们要明确什么是序列数据，摘取百度百科词条：时间序列数据是指在不同时间点上收集到的数据，这类数据反映了某一事物、现象等随时间的变化状态或程度。这是时间序列数据的定义，当然这里也可以不是时间，比如文字序列，但总归序列数据有一个特点——后面的数据跟前面的数据有关系。  </li></ul><p><strong>(1)  RNN的结构及变体</strong><br>&ensp;&ensp;&ensp;我们从基础的神经网络中知道，神经网络包含输入层、隐层、输出层，通过激活函数控制输出，层与层之间通过权值连接。激活函数是事先确定好的，那么神经网络模型通过训练“学“到的东西就蕴含在“权值“中。<br>&ensp;&ensp;&ensp;基础的神经网络只在层与层之间建立了权连接，RNN最大的不同之处就是在层之间的神经元之间也建立的权连接。如图<br><img src="https://github.com/HuanwenW/MyPostImag/blob/master/190408-NLP-gainian/RNN-1.png?raw=true" alt="Rnn"><span class="img-alt">Rnn</span>  </p><p>&ensp;&ensp;&ensp;这是一个标准的RNN结构图，图中每个箭头代表做一次变换，也就是说箭头连接带有权值。左侧是折叠起来的样子，右侧是展开的样子，左侧中h旁边的箭头代表此结构中的“循环“体现在隐层。<br>在展开结构中我们可以观察到，在标准的RNN结构中，隐层的神经元之间也是带有权值的。也就是说，随着序列的不断推进，前面的隐层将会影响后面的隐层。图中O代表输出，y代表样本给出的确定值，L代表损失函数，我们可以看到，“损失“也是随着序列的推荐而不断积累的。<br>除上述特点之外，标准RNN的还有以下特点：<br>1、权值共享，图中的W全是相同的，U和V也一样。<br>2、每一个输入值都只与它本身的那条路线建立权连接，不会和别的神经元连接。  </p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://www.atyun.com/30234.html" target="_blank" rel="noopener">一份详细的LSTM和GRU图解</a>  </li><li><a href="https://zhuanlan.zhihu.com/p/37070414" target="_blank" rel="noopener">Tensorflow实战(1): 实现深层循环神经网络</a>  </li><li><a href="https://x-algo.cn/index.php/2017/01/13/1609/" target="_blank" rel="noopener">从LSTM到Seq2Seq-大数据算法</a></li><li><a href="https://github.com/airalcorn2/Recurrent-Convolutional-Neural-Network-Text-Classifier" target="_blank" rel="noopener">GitHub - airalcorn2/Recurrent-Convolutional-Neural…</a>  </li><li><a href="https://github.com/zhangfazhan/TextRCNN" target="_blank" rel="noopener">GitHub - zhangfazhan/TextRCNN: TextRCNN 文本分类</a>  </li><li><a href="https://github.com/roomylee/rcnn-text-classification" target="_blank" rel="noopener">RCNN tf (推荐)</a>  </li><li><a href="https://blog.csdn.net/zhaojc1995/article/details/80572098" target="_blank" rel="noopener">RNN</a>  </li><li><a href="https://blog.csdn.net/heyongluoyao8/article/details/48636251" target="_blank" rel="noopener">循环神经网络(RNN, Recurrent Neural Networks)介绍</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;内容涵盖&quot;&gt;&lt;a href=&quot;#内容涵盖&quot; class=&quot;headerlink&quot; title=&quot;内容涵盖&quot;&gt;&lt;/a&gt;内容涵盖&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;RNN的结构。循环神经网络的提出背景、优缺点。着重学习RNN的反向传播、RNN出现的问题（梯度问题、长期依赖问题
      
    
    </summary>
    
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="RNN" scheme="http://yoursite.com/tags/RNN/"/>
    
  </entry>
  
  <entry>
    <title>Welcome to NLP-2019-huanhuan-homework-10</title>
    <link href="http://yoursite.com/2019/04/25/NLP-juanjishenjingwangluo-10/"/>
    <id>http://yoursite.com/2019/04/25/NLP-juanjishenjingwangluo-10/</id>
    <published>2019-04-25T08:00:35.000Z</published>
    <updated>2019-04-25T09:53:32.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="内容涵盖"><a href="#内容涵盖" class="headerlink" title="内容涵盖"></a>内容涵盖</h2><ul><li>卷积运算的定义、动机（稀疏权重、参数共享、等变表示）。一维卷积运算和二维卷积运算</li><li>反卷积(tf.nn.conv2d_transpose)</li><li>池化运算的定义、种类（最大池化、平均池化等）、动机</li><li>Text-CNN的原理</li><li>利用Text-CNN模型来进行文本分类  </li></ul><h2 id="1-卷积运算的定义、动机"><a href="#1-卷积运算的定义、动机" class="headerlink" title="1. 卷积运算的定义、动机"></a>1. 卷积运算的定义、动机</h2><p>&ensp;&ensp;&ensp;卷积网络也叫做卷积神经网络，是一种专门用来处理类似具有网格结构的数据的神经网络。例如时间序列数据（可以认为在时间轴上有规律的一维网格数据）、图像数据（可以认为二维的像素网格数据）。卷积神经网络在许多应用中发挥着巨大的作用，比如图像领域。卷积是一种特殊的线性运算，是对两个实值函数的一种数学运算，卷积运算通常用符号 * 来表示。卷积网络是指那些至少在网络的一层中使用卷积运算来代替一般的矩阵乘法运算的神经网络。</p><p>&ensp;&ensp;&ensp;卷积运算运用三个重要的思想来帮助改进机器学习系统 ：稀疏交互（sparse interactions)、参数共享（parameter sharing）、等变表示（equivariant pepresentations).</p><h3 id="1-卷积运算"><a href="#1-卷积运算" class="headerlink" title="(1)卷积运算"></a>(1)卷积运算</h3><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1.<a href="https://zhuanlan.zhihu.com/p/57575810" target="_blank" rel="noopener"><strong>卷积有多少种？</strong>一文读懂深度学习中的各种卷积</a><br>2.<a href="https://blog.csdn.net/yinkun6514/article/details/79281278" target="_blank" rel="noopener">卷积网络笔记</a><br>3.<a href="https://blog.csdn.net/AdamShan/article/details/79193775" target="_blank" rel="noopener">卷积神经网络入门，基于深度学习的车辆实时检测</a><br>4.<a href="https://blog.csdn.net/weixin_33774883/article/details/86942408" target="_blank" rel="noopener">卷积网络——动机</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;内容涵盖&quot;&gt;&lt;a href=&quot;#内容涵盖&quot; class=&quot;headerlink&quot; title=&quot;内容涵盖&quot;&gt;&lt;/a&gt;内容涵盖&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;卷积运算的定义、动机（稀疏权重、参数共享、等变表示）。一维卷积运算和二维卷积运算&lt;/li&gt;
&lt;li&gt;反卷积(tf
      
    
    </summary>
    
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="卷积神经网络基础" scheme="http://yoursite.com/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>Welcome to NLP-2019-huanhuan-homework-9</title>
    <link href="http://yoursite.com/2019/04/23/NLP-jiandanshenjingwangluo-9/"/>
    <id>http://yoursite.com/2019/04/23/NLP-jiandanshenjingwangluo-9/</id>
    <published>2019-04-23T13:34:24.000Z</published>
    <updated>2019-04-23T14:24:20.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="内容涵盖"><a href="#内容涵盖" class="headerlink" title="内容涵盖"></a>内容涵盖</h2><ul><li>文本表示：从one-hot到word2vec。  </li><li>词袋模型：离散、高维、稀疏。  </li><li>分布式表示：连续、低维、稠密。word2vec词向量原理并实践，用来表示文本。  </li></ul><h2 id="1-文本表示"><a href="#1-文本表示" class="headerlink" title="1.文本表示"></a>1.文本表示</h2><p>&ensp;&ensp;&ensp;词向量的意思就是通过一个数字组成的向量来表示一个词，这个向量的构成有很多种方法，如one-hot编码、基于共现矩阵的方式、word2vec、动态词向量ELMo等。  </p><p><strong>(1)one-hot编码</strong><br>&ensp;&ensp;&ensp;one-hot编码又称独热编码、一位有效编码。其方法是使用N位状态寄存器来对N个状态进行编码，每个状态都有它独立的寄存器位，并且在任意时候，其中只有一位有效。<br>假设在一个语料集合中，一共有n个不同的词，则可以使用一个长度为n的向量，对于第i个词（i=0….n-1）,向量index=i处的值为1外，向量其他位置的值都为0，这样就可以唯一的通过[0,0,0,1….0,0]形式的向量表示一个词。one-hot向量比较简单也容易理解，但是有很多问题。比如，加入新词时，整个向量的长度会改变，并且存在维数过高难以计算的问题，以及向量的表示方法很难体现两个词之间的关系，因此一般情况下one-hot向量较少使用。<br><strong>优势：</strong>简单易懂<br><strong>不足：</strong> 维度灾难、词汇鸿沟（向量之间都是孤立的）  </p><p><strong>（2）基于贡献矩阵的方式</strong>    </p><p><img src="https://github.com/HuanwenW/MyPostImag/blob/master/190408-NLP-gainian/gongxiangjuzheng.png?raw=true" alt="gongxiangjuzheng"><span class="img-alt">gongxiangjuzheng</span>  </p><p><img src="https://github.com/HuanwenW/MyPostImag/blob/master/190408-NLP-gainian/gongxiangjuzheng-1.png?raw=true" alt="juli"><span class="img-alt">juli</span></p><p>上述矩阵是一个n*n的对称矩阵X，矩阵维数随着词典数量n的增大而增大，可以使用奇异值分解SVD将矩阵维度降低。但是仍存在问题：</p><ol><li>矩阵X的维度经常改</li><li>由于大部分词并不共现而导致的稀疏性</li><li>矩阵维度过高带来的高计算复杂度</li></ol><p><strong>（3）基于神经网络的方式（world embedding）：world2vec</strong><br>&ensp;&ensp;&ensp;Embedding层（输入层到隐藏层）是以one hot为输入、中间层节点数为词向量维数的全连接层，这个全连接层的参数就是我们要获取的词向量表！  </p><h2 id="2-词袋模型：离散、高维、稀疏。"><a href="#2-词袋模型：离散、高维、稀疏。" class="headerlink" title="2. 词袋模型：离散、高维、稀疏。"></a>2. 词袋模型：离散、高维、稀疏。</h2><h2 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h2><ol start="0"><li><a href="https://blog.csdn.net/weixin_38493025/article/details/85245044" target="_blank" rel="noopener">词向量（从one-hot到word2vec）</a></li><li><a href="https://blog.csdn.net/itplus/article/details/37969519" target="_blank" rel="noopener">word2vec 中的数学原理详解</a>    </li><li><a href="http://www.hankcs.com/nlp/word2vec.html" target="_blank" rel="noopener">word2vec原理推导与代码分析</a>  </li><li><a href="https://github.com/facebookresearch/fastText#building-fasttext-for-python" target="_blank" rel="noopener">word2vec3：word2vec中的数学原理详解（四）基于 Hierarchical Softmax 的模型</a>  </li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;内容涵盖&quot;&gt;&lt;a href=&quot;#内容涵盖&quot; class=&quot;headerlink&quot; title=&quot;内容涵盖&quot;&gt;&lt;/a&gt;内容涵盖&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;文本表示：从one-hot到word2vec。  &lt;/li&gt;
&lt;li&gt;词袋模型：离散、高维、稀疏。  &lt;/li&gt;
      
    
    </summary>
    
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="简单神经网络" scheme="http://yoursite.com/tags/%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Welcome to NLP-2019-huanhuan-homework-8</title>
    <link href="http://yoursite.com/2019/04/21/NLP-shenjingwangluojichu-8/"/>
    <id>http://yoursite.com/2019/04/21/NLP-shenjingwangluojichu-8/</id>
    <published>2019-04-21T10:52:33.000Z</published>
    <updated>2019-04-21T12:25:24.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="内容涵盖"><a href="#内容涵盖" class="headerlink" title="内容涵盖"></a>内容涵盖</h2><ul><li>前馈神经网络、网络层数、输入层、隐藏层、输出层、隐藏单元、激活函数的概念。</li><li>感知机相关；利用tensorflow等工具定义简单的几层网络（激活函数sigmoid），递归使用链式法则来实现反向传播。</li><li>激活函数的种类以及各自的提出背景、优缺点。（和线性模型对比，线性模型的局限性，去线性化）</li><li>深度学习中的正则化（参数范数惩罚：L1正则化、L2正则化；数据集增强；噪声添加；early stop；Dropout层）、正则化的介绍。</li><li>深度模型中的优化：参数初始化策略；自适应学习率算法（梯度下降、AdaGrad、RMSProp、Adam；优化算法的选择）；batch norm层（提出背景、解决什么问题、层在训练和测试阶段的计算公式）；layer norm层。  </li></ul><h2 id="1-神经网络基础概念"><a href="#1-神经网络基础概念" class="headerlink" title="1. 神经网络基础概念"></a>1. 神经网络基础概念</h2><p>&ensp;&ensp;&ensp; 神经网络是机器学习中的一种模型，是一种模仿动物神经网络行为特征，进行分布式并行信息处理的算法数学模型。神经网络最开始是受生物神经系统的启发，为了模拟生物神经系统而出现的。生物神经系统中最基本的计算单元是神经元。<br><img src="https://github.com/HuanwenW/MyPostImag/blob/master/190408-NLP-gainian/shenjingwangluo-0.jpg?raw=true" alt="shengwu"><span class="img-alt">shengwu</span>  </p><h3 id="1-1-神经网络模型"><a href="#1-1-神经网络模型" class="headerlink" title="1.1 神经网络模型"></a>1.1 神经网络模型</h3><p>&ensp;&ensp;&ensp; 所谓神经网络就是将许多个单一“神经元”联结在一起，这样，一个“神经元”的输出就可以是另一个“神经元”的输入。例如，下图就是一个简单的神经网络：<br><img src="https://github.com/HuanwenW/MyPostImag/blob/master/190408-NLP-gainian/shenjingwangluo-1.jpg?raw=true" alt="shenjingwangluo"><span class="img-alt">shenjingwangluo</span></p><h2 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h2><ol><li><a href="https://blog.csdn.net/SMith7412/article/details/88396674" target="_blank" rel="noopener">人工神经网络知识、激活函数、正则化、优化技术、Batch Normalization、Layer Normalization</a>  </li><li><a href="https://blog.csdn.net/qq_36047533/article/details/88419931" target="_blank" rel="noopener">神经网络基础</a>  </li><li><a href></a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;内容涵盖&quot;&gt;&lt;a href=&quot;#内容涵盖&quot; class=&quot;headerlink&quot; title=&quot;内容涵盖&quot;&gt;&lt;/a&gt;内容涵盖&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;前馈神经网络、网络层数、输入层、隐藏层、输出层、隐藏单元、激活函数的概念。&lt;/li&gt;
&lt;li&gt;感知机相关；利用t
      
    
    </summary>
    
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="Network" scheme="http://yoursite.com/tags/Network/"/>
    
  </entry>
  
  <entry>
    <title>Welcome to NLP-2019-huanhuan-homework-7</title>
    <link href="http://yoursite.com/2019/04/19/LDA/"/>
    <id>http://yoursite.com/2019/04/19/LDA/</id>
    <published>2019-04-19T08:52:33.000Z</published>
    <updated>2019-04-19T09:10:16.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="内容涵盖"><a href="#内容涵盖" class="headerlink" title="内容涵盖"></a>内容涵盖</h2><ul><li>pLSA、共轭先验分布；LDA主题模型原理</li><li>LDA应用场景 </li><li>LDA优缺点 </li><li>LDA 参数学习 </li><li>使用LDA生成主题特征，在之前特征的基础上加入主题特征进行文本分类  </li></ul><h2 id="LSA"><a href="#LSA" class="headerlink" title="LSA"></a>LSA</h2><p>&ensp;&ensp;&ensp;LSA(latent semantic analysis)潜在语义分析，也被称为 LSI(latent semantic index)，是 Scott Deerwester, Susan T. Dumais 等人在 1990 年提出来的一种新的索引和检索方法。该方法和传统向量空间模型(vector space model)一样使用向量来表示词(terms)和文档(documents)，并通过向量间的关系(如夹角)来判断词及文档间的关系；不同的是，LSA 将词和文档映射到潜在语义空间，从而去除了原始向量空间中的一些“噪音”，提高了信息检索的精确度。</p><h2 id="PLSA"><a href="#PLSA" class="headerlink" title="PLSA"></a>PLSA</h2><p>&ensp;&ensp;&ensp;概率隐语义分析（PLSA）是一个著名的针对文本建模的模型，是一个生成模型。因为加入了主题模型，所以可以很大程度上改善多词一义和一词多义的问题。Hoffmm在1999年提出了概率隐语义分析（Probabilistic Latent Semantic Analysis）。他认为每个主题下都有一个词汇的概率分布，而一篇文章通常由多个主题构成，并且文章中的每个单词都是由某个主题生成的。</p><h2 id="LDA"><a href="#LDA" class="headerlink" title="LDA"></a>LDA</h2><p>&ensp;&ensp;&ensp;LDA（Latent Dirichlet Allocation）是一种文档主题生成模型，也称为一个三层贝叶斯概率模型，包含词、主题和文档三层结构。所谓生成模型，就是说，我们认为一篇文章的每个词都是通过“以一定概率选择了某个主题，并从这个主题中以一定概率选择某个词语”这样一个过程得到。文档到主题服从多项式分布，主题到词服从多项式分布。</p><p>&ensp;&ensp;&ensp;LDA是一种非监督机器学习技术，可以用来识别大规模文档集（document collection）或语料库（corpus）中潜藏的主题信息。它采用了词袋（bag of words）的方法，这种方法将每一篇文档视为一个词频向量，从而将文本信息转化为了易于建模的数字信息。但是词袋方法没有考虑词与词之间的顺序，这简化了问题的复杂性，同时也为模型的改进提供了契机。每一篇文档代表了一些主题所构成的一个概率分布，而每一个主题又代表了很多单词所构成的一个概率分布。</p><p>&ensp;&ensp;&ensp; LSA（Latent semantic analysis，隐性语义分析）、pLSA（Probabilistic latent semantic analysis，概率隐性语义分析）和 LDA（Latent Dirichlet allocation，隐狄利克雷分配）这三种模型都可以归类到话题模型（Topic model，或称为主题模型）中。相对于比较简单的向量空间模型，主题模型通过引入主题这个概念，更进一步地对文本进行语义层面上的理解</p><h2 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h2><ol><li><a href="https://blog.csdn.net/yyy430/article/details/88346920" target="_blank" rel="noopener">朴素贝叶斯 &amp; SVM &amp; LDA文本分类</a>  </li><li><a href="https://blog.csdn.net/chen_yiwei/article/details/88370526" target="_blank" rel="noopener">LDA主题模型</a></li><li><a href="http://www.cnblogs.com/bentuwuying/p/6219970.html" target="_blank" rel="noopener">LSA，pLSA原理及其代码实现</a>  </li><li><a href="https://blog.csdn.net/qq_39422642/article/details/78730662" target="_blank" rel="noopener">主题模型（LDA）(一)–通俗理解与简单应用</a>  </li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;内容涵盖&quot;&gt;&lt;a href=&quot;#内容涵盖&quot; class=&quot;headerlink&quot; title=&quot;内容涵盖&quot;&gt;&lt;/a&gt;内容涵盖&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;pLSA、共轭先验分布；LDA主题模型原理&lt;/li&gt;
&lt;li&gt;LDA应用场景 &lt;/li&gt;
&lt;li&gt;LDA优缺点 
      
    
    </summary>
    
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="LDA" scheme="http://yoursite.com/tags/LDA/"/>
    
  </entry>
  
  <entry>
    <title>谱聚类（Spectral Clustering）算法概念及应用</title>
    <link href="http://yoursite.com/2019/04/16/Pujulei/"/>
    <id>http://yoursite.com/2019/04/16/Pujulei/</id>
    <published>2019-04-16T07:42:53.000Z</published>
    <updated>2019-04-17T07:30:04.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>&ensp;&ensp;&ensp;在研究图数据管理方面论文时候，《Label Informed Attributed Network Embedding》文章中提到对图结构表示用的是谱聚类方法，特来了解下该算法的原理。  </p><h2 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h2><p><a href="https://blog.csdn.net/gshgsh1228/article/details/52199870" target="_blank" rel="noopener">正则化1</a><br><a href="https://blog.csdn.net/haima1998/article/details/79425831" target="_blank" rel="noopener">正则化2</a><br><a href="https://blog.csdn.net/qq_14959801/article/details/81056025" target="_blank" rel="noopener">正则化3</a></p><p><a href="http://blog.pluskid.org/?page_id=683" target="_blank" rel="noopener">SVM</a>  </p><p>00.<a href="http://blog.pluskid.org/?p=287" target="_blank" rel="noopener">漫谈 Clustering (4): Spectral Clustering</a><br>00.<a href="https://blog.csdn.net/qq_30159015/article/details/83271065" target="_blank" rel="noopener">拉普拉斯矩阵（Laplacian matrix）</a><br>01.<a href="https://blog.csdn.net/Broccoli_Lian/article/details/79755225" target="_blank" rel="noopener">关联矩阵，邻接矩阵，拉普拉斯矩阵</a></p><p>0.<a href="https://blog.csdn.net/u012771351/article/details/53213993" target="_blank" rel="noopener">聚类系列-谱聚类</a><br>1.<a href="https://blog.csdn.net/qq_24519677/article/details/82291867" target="_blank" rel="noopener"><strong>谱聚类（Spectral Clustering）算法介绍</strong></a>  </p><ol start="2"><li><a href="https://www.cnblogs.com/pinard/p/6221564.html" target="_blank" rel="noopener"><strong>谱聚类（spectral clustering）原理总结</strong></a>  </li><li><a href="https://blog.csdn.net/u012500237/article/details/72864258" target="_blank" rel="noopener">聚类系列-谱聚类</a>  </li><li><a href="https://blog.csdn.net/zhangyi880405/article/details/39781817" target="_blank" rel="noopener">谱聚类算法详解</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;在研究图数据管理方面论文时候，《Label Informed Attributed Network Em
      
    
    </summary>
    
    
      <category term="技术篇" scheme="http://yoursite.com/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="聚类" scheme="http://yoursite.com/tags/%E8%81%9A%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>Welcome to NLP-2019-huanhuan-homework-5</title>
    <link href="http://yoursite.com/2019/04/15/NLP-homework-5-Pushubeiyesi/"/>
    <id>http://yoursite.com/2019/04/15/NLP-homework-5-Pushubeiyesi/</id>
    <published>2019-04-15T11:00:35.000Z</published>
    <updated>2019-04-15T14:19:42.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="内容涵盖"><a href="#内容涵盖" class="headerlink" title="内容涵盖"></a>内容涵盖</h2><ul><li><strong>朴素贝叶斯的原理</strong>  </li><li><strong>朴素贝叶斯应用场景</strong>  </li><li><strong>朴素贝叶斯优缺点</strong>  </li><li><strong>朴素贝叶斯 sklearn 参数学习</strong>   </li><li><strong>利用朴素贝叶斯模型结合 Tf-idf 算法进行文本分类</strong>  </li></ul><h2 id="0-知识回顾"><a href="#0-知识回顾" class="headerlink" title="0.知识回顾"></a>0.知识回顾</h2><p><strong>(1) 联合概率</strong>  </p><p>&ensp;&ensp;&ensp;P(x)表示x发生的概率，P(y)表示y发生的概率，则 x，y 同时发生的概率为：  </p><p><img src="http://latex.codecogs.com/gif.latex?P(x,y)=P(x%7Cy)P(y)=P(y%7Cx)P(x)" alt></p><p>&ensp;&ensp;&ensp;特别的，当x，y独立时，上式可以写作：   </p><p><strong><img src="http://latex.codecogs.com/gif.latex?P(x,y)=P(x)P(y)" alt></strong>    </p><p>&ensp;&ensp;&ensp;原因在于，当x，y独立时，即x的发生与y的发生不相关，因此就有p(x|y)=p(x)，同理p(y|x)=p(y)。</p><p><strong>(2) 条件概率</strong> （是联合概率的变形，即把 条件事件 放在了等式左边）   </p><p><strong><img src="http://latex.codecogs.com/gif.latex?P(x%7Cy)=%5Cfrac%7BP(x,y)%7D%7BP(y)%7D" alt></strong>    </p><p> <strong><img src="http://latex.codecogs.com/gif.latex?P(y%7Cx)=%5Cfrac%7BP(x,y)%7D%7BP(x)%7D" alt></strong>    </p><p><strong>(3) 全概率</strong>   </p><p><strong><img src="http://latex.codecogs.com/gif.latex?P(x)=%5Csum_%7Bi=1%7D%5E%7BM%7DP(x%7Cy_%7Bi%7D)P(y_%7Bi%7D)" alt></strong>   </p><p>其中，<strong><img src="http://latex.codecogs.com/gif.latex?%5Csum_%7Bi=1%7D%5E%7BM%7DP(y_%7Bi%7D)" alt> = 1</strong> ，也就是说对于y的M种（所有）情况都要考虑到。</p><p><strong>(3) 贝叶斯</strong>   </p><p>贝叶斯理论指的是，根据一个已发生事件的概率，计算另一个事件的发生概率,<strong>贝叶斯公式：</strong>   </p><p><img src="http://latex.codecogs.com/gif.latex?P(y_%7Bi%7D%7Cx)=%5Cfrac%7BP(y_%7Bi%7D)P(y_%7Bi%7D%7Cx)%7D%7B%5Csum_%7Bj=1%7D%5E%7Bn%7DP(y_%7Bj%7D)P(x%7Cy_%7Bj%7D)%7D" alt></p><p>等同于我们经常简略表达：</p><p><strong><img src="http://latex.codecogs.com/gif.latex?P(Y%7CX)=%5Cfrac%7BP(X%7CY)P(Y)%7D%7BP(X)%7D" alt></strong></p><p>其中，<strong>X</strong>: 特征向量 &ensp;&ensp;&ensp; <strong>Y</strong>：类别<br>&ensp;&ensp;&ensp;&ensp; <strong>先验概率P(X)</strong>：是指根据以往经验和分析得到的概率。<br>&ensp;&ensp;&ensp;&ensp;<strong>后验概率P(Y|X)</strong> ：事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小。<br>&ensp;&ensp;&ensp;&ensp;<strong>类条件概率P(X|Y)</strong>：在已知某类别的特征空间中，出现特征值 <strong>X</strong> 的概率密度。 </p><h2 id="1-朴素贝叶斯原理"><a href="#1-朴素贝叶斯原理" class="headerlink" title="1.朴素贝叶斯原理"></a>1.朴素贝叶斯原理</h2><p>&ensp;&ensp;&ensp;贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。而<strong>朴素贝叶斯分类是贝叶斯分类中最简单，也是常见的一种分类方法</strong>。</p><p><strong>朴素贝叶斯（naïve beyes）</strong> 法是基于 <strong>贝叶斯定理</strong> 与 <strong>特征条件独立假设</strong>的分类方法。—by李航《统计学习》</p><p>基于朴素贝叶斯公式，比较出后验概率的最大值来进行分类，后验概率的计算是由先验概率与类条件概率的乘积得出，先验概率和类条件概率要通过训练数据集得出，即为朴素贝叶斯分类模型，将其保存为中间结果，测试文档进行分类时调用这个中间结果得出后验概率。  </p><p>&ensp;&ensp;&ensp;</p><h2 id="2-朴素贝叶斯应用场景"><a href="#2-朴素贝叶斯应用场景" class="headerlink" title="2. 朴素贝叶斯应用场景"></a>2. 朴素贝叶斯应用场景</h2><h2 id="3-朴素贝叶斯优缺点"><a href="#3-朴素贝叶斯优缺点" class="headerlink" title="3.朴素贝叶斯优缺点"></a>3.朴素贝叶斯优缺点</h2><ul><li><strong>优点</strong>  </li></ul><p>(1) 算法逻辑简单,易于实现；<br>(2) 分类过程中时空开销小（假设特征相互独立，只会涉及到二维存储）</p><ul><li><strong>缺点</strong>  </li></ul><p>&ensp;&ensp;&ensp;理论上，朴素贝叶斯模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是因为朴素贝叶斯模型<strong>假设属性之间相互独立</strong>，这个假设在实际应用中往往是不成立的，在属性个数比较多或者属性之间相关性较大时，分类效果不好。  </p><h2 id="4-朴素贝叶斯-sklearn-参数学习"><a href="#4-朴素贝叶斯-sklearn-参数学习" class="headerlink" title="4.朴素贝叶斯 sklearn 参数学习"></a>4.朴素贝叶斯 sklearn 参数学习</h2><h2 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h2><ol><li><a href="https://www.cnblogs.com/pinard/p/6069267.html" target="_blank" rel="noopener"><strong>朴素贝叶斯相关的统计学知识</strong></a></li><li><a href="https://blog.csdn.net/qq_35044025/article/details/79322169" target="_blank" rel="noopener"><strong>sklearn的机器学习之路：朴素贝叶斯</strong></a>  </li><li><a href="http://www.52nlp.cn/%E7%90%86%E8%AE%BA-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%A8%A1%E5%9E%8B%E7%AE%97%E6%B3%95%E7%A0%94%E7%A9%B6%E4%B8%8E%E5%AE%9E%E4%BE%8B%E5%88%86%E6%9E%90#more-10451" target="_blank" rel="noopener"><strong>朴素贝叶斯模型算法研究与实例分析</strong></a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;内容涵盖&quot;&gt;&lt;a href=&quot;#内容涵盖&quot; class=&quot;headerlink&quot; title=&quot;内容涵盖&quot;&gt;&lt;/a&gt;内容涵盖&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;朴素贝叶斯的原理&lt;/strong&gt;  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;朴素贝叶斯应用场景&lt;/
      
    
    </summary>
    
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="朴素贝叶斯" scheme="http://yoursite.com/tags/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    
  </entry>
  
  <entry>
    <title>Welcome to NLP-2019-huanhuan-homework-6</title>
    <link href="http://yoursite.com/2019/04/13/NLP-homework-6/"/>
    <id>http://yoursite.com/2019/04/13/NLP-homework-6/</id>
    <published>2019-04-13T10:52:33.000Z</published>
    <updated>2019-04-17T12:11:36.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="内容涵盖"><a href="#内容涵盖" class="headerlink" title="内容涵盖"></a>内容涵盖</h1><ul><li>SVM的原理</li><li>SVM应用场景 </li><li>SVM优缺点 </li><li>SVM sklearn 参数学习 </li><li>利用SVM模型结合 Tf-idf 算法进行文本分类  </li></ul><h2 id="SVM的原理"><a href="#SVM的原理" class="headerlink" title="SVM的原理"></a>SVM的原理</h2><p>支持向量机（SVM）算法基于结构风险最小化原理，将数据集合压缩到支持向量集合，学习得到分类决策函数。这种技术解决了以往需要无穷大样本数量的问题，它只需要将一定数量的文本通过计算抽象成向量化的训练文本数据，提高了分类的精确率。支持向量机（SVM）算法是根据有限的样本信息，在模型的复杂性与学习能力之间寻求最佳折中，以求获得最好的推广能力支持向量机算法.  </p><h2 id="SVM优缺点"><a href="#SVM优缺点" class="headerlink" title="SVM优缺点"></a>SVM优缺点</h2><ol><li>专门针对有限样本情况，其目标是得到现有信息下的最优解而不仅仅是样本数量趋于无穷大时的最优值；  </li><li>算法最终转化为一个二次型寻优问题，理论上得到的是全局最优点，解决了在神经网络方法中无法避免的局部极值问题；  </li><li>支持向量机算法能同时适用于稠密特征矢量与稀疏特征矢量两种情况，而其他一些文本分类算法不能同时满足两种情况；  </li><li>支持向量机算法能够找出包含重要分类信息的支持向量，是强有力的增量学习和主动学习工具，在文本分类中具有很大的应用潜力。  </li></ol><h2 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h2><ol><li><a href="https://blog.csdn.net/yyy430/article/details/88346920" target="_blank" rel="noopener">朴素贝叶斯 &amp; SVM &amp; LDA文本分类</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;内容涵盖&quot;&gt;&lt;a href=&quot;#内容涵盖&quot; class=&quot;headerlink&quot; title=&quot;内容涵盖&quot;&gt;&lt;/a&gt;内容涵盖&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;SVM的原理&lt;/li&gt;
&lt;li&gt;SVM应用场景 &lt;/li&gt;
&lt;li&gt;SVM优缺点 &lt;/li&gt;
&lt;li&gt;SVM s
      
    
    </summary>
    
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="SVM" scheme="http://yoursite.com/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>Welcome to NLP-2019-huanhuan-homework-4</title>
    <link href="http://yoursite.com/2019/04/13/NLP-homework-3/"/>
    <id>http://yoursite.com/2019/04/13/NLP-homework-3/</id>
    <published>2019-04-13T10:52:33.000Z</published>
    <updated>2019-09-13T04:33:59.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="内容涵盖"><a href="#内容涵盖" class="headerlink" title="内容涵盖"></a>内容涵盖</h2><ul><li><strong>TF-IDF原理</strong></li><li><strong>文本矩阵化，使用词袋模型，以TF-IDF特征值为权重</strong>（可以使用Python中TfidfTransformer库）</li><li><strong>互信息的原理</strong></li><li><strong>使用第二步生成的特征矩阵，利用互信息进行特征筛选</strong></li></ul><h2 id="1-TF-IDF原理"><a href="#1-TF-IDF原理" class="headerlink" title="1.TF-IDF原理"></a>1.TF-IDF原理</h2><p><strong>(1) 定义</strong><br>&ensp;&ensp;&ensp; TF-IDF(Term Frequency - Inverse Document Frequency)，即“词频-逆文本频率”。是一种用于资讯检索与资讯探勘的常用加权技术。TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。<strong>字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降.</strong>  </p><p><strong>(2)思想</strong><br>&ensp;&ensp;&ensp;如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。<strong>TF-IDF实际上就是 TF X IDF，其中TF表示词条在文章Document 中出现的频率；IDF其主要思想就是，如果包含某个词 Word的文档越少，则这个词的区分度就越大，也就是 IDF 越大。</strong> 对于如何获取一篇文章的关键词，我们可以计算这边文章出现的所有名词的 TF-IDF，TF-IDF越大，则说明这个名词对这篇文章的区分度就越高，取 TF-IDF 值较大的几个词，就可以当做这篇文章的关键词。</p><p><strong>(3) 计算步骤</strong><br><strong>a. 计算词频(TF)</strong>  </p><p><strong><img src="http://latex.codecogs.com/gif.latex?tf_%7Bi,j%7D=%5Cfrac%7Bn_%7Bi,j%7D%7D%7B%5Csum_%7B0%7D%5E%7Bk%7Dn_%7Bk,j%7D%7D" alt></strong>    </p><p>分子是该词在文件 dj 中的出现次数，而分母  则是在文件 dj 中所有字词的出现次数之和。</p><p><strong>b. 计算逆文档频率（IDF）</strong>   </p><h2 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h2><ol><li><a href="https://www.cnblogs.com/pinard/p/6693230.html" target="_blank" rel="noopener"><strong>文本挖掘预处理之TF-IDF</strong></a></li><li><a href="https://blog.csdn.net/lionel_fengj/article/details/53699903" target="_blank" rel="noopener"><strong>自然语言处理系列之TF-IDF算法</strong></a></li><li><a href="http://www.cnblogs.com/biyeymyhjob/archive/2012/07/17/2595249.html" target="_blank" rel="noopener"><strong>TF-IDF及其算法</strong></a></li><li><a href="https://www.jianshu.com/p/f3b92124cd2b" target="_blank" rel="noopener"><strong>使用不同的方法计算TF-IDF值</strong></a></li><li><a href="https://blog.csdn.net/u013710265/article/details/72848755" target="_blank" rel="noopener"><strong>sklearn-点互信息和互信息</strong></a></li><li><a href="https://baijiahao.baidu.com/s?id=1604074325918456186&wfr=spider&for=pc" target="_blank" rel="noopener"><strong>如何进行特征选择（理论篇）机器学习你会遇到的“坑”</strong></a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;内容涵盖&quot;&gt;&lt;a href=&quot;#内容涵盖&quot; class=&quot;headerlink&quot; title=&quot;内容涵盖&quot;&gt;&lt;/a&gt;内容涵盖&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;TF-IDF原理&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;文本矩阵化，使用词袋模型，
      
    
    </summary>
    
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="TF-IDF" scheme="http://yoursite.com/tags/TF-IDF/"/>
    
  </entry>
  
  <entry>
    <title>Welcome to NLP-2019-huanhuan-homework-3</title>
    <link href="http://yoursite.com/2019/04/11/tezhentiqu/"/>
    <id>http://yoursite.com/2019/04/11/tezhentiqu/</id>
    <published>2019-04-11T12:44:17.000Z</published>
    <updated>2019-04-12T08:02:42.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="内容涵盖"><a href="#内容涵盖" class="headerlink" title="内容涵盖"></a>内容涵盖</h2><ul><li>分词的正向最大、逆向最大、双向最大匹配法概念；</li><li>词、字符频率统计；（可以使用Python中的collections.Counter模块，也可以自己寻找其他好用的库）</li><li>语言模型中unigram、bigram、trigram的概念；</li><li>unigram、bigram频率统计；（可以使用Python中的collections.Counter模块，也可以自己寻找其他好用的库）</li><li>文本矩阵化：要求采用词袋模型且是词级别的矩阵化<br>步骤有：<br>3.1 分词（可采用结巴分词来进行分词操作，其他库也可以）；<br>3.2 去停用词；构造词表。<br>3.3 每篇文档的向量化。  </li></ul><h2 id="1-基本文本处理技能"><a href="#1-基本文本处理技能" class="headerlink" title="1. 基本文本处理技能"></a>1. 基本文本处理技能</h2><p>&ensp;&ensp;&ensp;Python中分分词工具很多，包括盘古分词、Yaha分词、Jieba分词、清华THULAC等。它们的基本用法都大同小异，本文以结巴分词 为例.  </p><p><strong>(1) 分词算法设计中的几个基本原则：</strong>  </p><p><strong>a.</strong>颗粒度越大越好：用于进行语义分析的文本分词，要求分词结果的颗粒度越大，即单词的字数越多，所能表示的含义越确切，如：“公安局长”可以分为“公安 局长”、“公安局 长”、“公安局长”都算对，但是要用于语义分析，则“公安局长”的分词结果最好（当然前提是所使用的词典中有这个词）</p><p><strong>b.</strong>切分结果中非词典词越少越好，单字字典词数越少越好，这里的“非词典词”就是不包含在词典中的单字，而“单字字典词”指的是可以独立运用的单字，如“的”、“了”、“和”、“你”、“我”、“他”。例如：“技术和服务”，可以分为“技术 和服 务”以及“技术 和 服务”，但“务”字无法独立成词（即词典中没有），但“和”字可以单独成词（词典中要包含），因此“技术 和服 务”有1个非词典词，而“技术 和 服务”有0个非词典词，因此选用后者。</p><p><strong>c.</strong>总体词数越少越好，在相同字数的情况下，总词数越少，说明语义单元越少，那么相对的单个语义单元的权重会越大，因此准确性会越高。</p><p><strong>(2) 匹配法：</strong><br>&ensp;&ensp;&ensp;最大匹配是指以词典为依据，取词典中最长单词为第一个次取字数量的扫描串，在词典中进行扫描（为提升扫描效率，还可以跟据字数多少设计多个字典，然后根据字数分别从不同字典中进行扫描）。例如：词典中最长词为“我爱北京天安门”共7个汉字，则最大匹配起始字数为7个汉字。然后逐字递减，在对应的词典中进行查找。<br><font color="#8A2BE2" size="3"><strong>&ensp;&ensp;下面以“我们在野生动物园玩”详细说明一下这几种匹配方法：</strong></font>       </p><p><strong>a. 正向最大匹配法</strong>  </p><p><strong>正向即从前往后取词，从7-&gt;1，每次减一个字，直到词典命中或剩下1个单字。</strong>   </p><p>第1次：“我们在野生动物”，扫描7字词典，无<br>第2次：“我们在野生动”，扫描6字词典，无<br><strong>……</strong><br>第6次：“我们”，扫描2字词典，有  </p><p><strong>扫描中止，输出第1个词为“我们”，去除第1个词后开始第2轮扫描，即：</strong>  </p><p>第1次：“在野生动物园玩”，扫描7字词典，无<br>第2次：“在野生动物园”，扫描6字词典，无<br><strong>……</strong><br>第6次：“在野”，扫描2字词典，有  </p><p><strong>扫描中止，输出第2个词为“在野”，去除第2个词后开始第3轮扫描，即：</strong>    </p><p>第1次：“生动物园玩”，扫描5字词典，无<br><strong>……</strong><br>第4次：“生动”，扫描2字词典，有</p><p><strong>扫描中止，输出第3个词为“生动”，第4轮扫描，即：</strong>    </p><p>第1次：“物园玩”，扫描3字词典，无<br>第2次：“物园”，扫描2字词典，无<br>第3次：“物”，扫描1字词典，无  </p><p><strong>扫描中止，输出第4个词为“物”，非字典词数加1，开始第5轮扫描，即：</strong>    </p><p>第1次：“园玩”，扫描2字词典，无<br>第2次：“园”，扫描1字词典，有  </p><p><strong>扫描中止，输出第5个词为“园”，单字字典词数加1，开始第6轮扫描，即：</strong>    </p><p>第1次：“玩”，扫描1字字典词，有</p><p><strong>扫描中止，输出第6个词为“玩”，单字字典词数加1，整体扫描结束。</strong>   </p><p><font color="#8A2BE2" size="3">*<em>正向最大匹配法，最终切分结果为：“我们/在野/生动/物/园/玩”，其中，单字字典词为2，非词典词为1。  *</em></font>   </p><p>*<em>b.逆向最大匹配法  *</em>  </p><p><strong>逆向即从后往前取词，其他逻辑和正向相同。即：</strong></p><p>第1轮扫描：“在野生动物园玩”<br>第1次：“在野生动物园玩”，扫描7字词典，无<br>第2次：“野生动物园玩”，扫描6字词典，无<br><strong>……</strong><br>第7次：“玩”，扫描1字词典，有  </p><p><strong>扫描中止，输出“玩”，单字字典词加1，开始第2轮扫描</strong>  </p><p>第2轮扫描：“们在野生动物园”<br>第1次：“们在野生动物园”，扫描7字词典，无<br>第2次：“在野生动物园”，扫描6字词典，无<br>第3次：“野生动物园”，扫描5字词典，有  </p><p><strong>扫描中止，输出“野生动物园”，开始第3轮扫描</strong></p><p>第3轮扫描：“我们在”<br>第1次：“我们在”，扫描3字词典，无<br>第2次：“们在”，扫描2字词典，无<br>第3次：“在”，扫描1字词典，有  </p><p><strong>扫描中止，输出“在”，单字字典词加1，开始第4轮扫描</strong></p><p>第4轮扫描：“我们”<br>第1次：“我们”，扫描2字词典，有  </p><p><strong>扫描中止，输出“我们”，整体扫描结束。</strong></p><p><font color="#8A2BE2" size="3"><strong>逆向最大匹配法，最终切分结果为：“我们/在/野生动物园/玩”，其中，单字字典词为2，非词典词为0。</strong></font>    </p><p><strong>c.双向最大匹配法</strong>     </p><p>&ensp;&ensp;&ensp;正向最大匹配法和逆向最大匹配法，都有其局限性，我举得例子是正向最大匹配法局限性的例子，逆向也同样存在（如：长春药店，逆向切分为“长/春药店”），因此有人又提出了双向最大匹配法，双向最大匹配法。即，两种算法都切一遍，然后根据大颗粒度词越多越好，非词典词和单字词越少越好的原则，选取其中一种分词结果输出。  </p><p>如：“我们在野生动物园玩”  </p><p>正向最大匹配法，最终切分结果为：“我们/在野/生动/物/园/玩”，其中，两字词3个，单字字典词为2，非词典词为1。  </p><p>逆向最大匹配法，最终切分结果为：“我们/在/野生动物园/玩”，其中，五字词1个，两字词1个，单字字典词为2，非词典词为0。  </p><p>非字典词：正向(1)&gt;逆向(0)（越少越好）  </p><p>单字字典词：正向(2)=逆向(2)（越少越好）  </p><p>总词数：正向(6)&gt;逆向(4)（越少越好）  </p><p>因此最终输出为逆向结果  </p><h2 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h2><ol><li><a href="https://github.com/fxsjy/jieba" target="_blank" rel="noopener">结巴中文分词理论+实践</a></li><li><a href="http://blog.sina.com.cn/s/blog_53daccf401011t74.html" target="_blank" rel="noopener">中文分词基础原则及正向最大匹配法、逆向最大匹配法、双向最大匹配法的分析</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;内容涵盖&quot;&gt;&lt;a href=&quot;#内容涵盖&quot; class=&quot;headerlink&quot; title=&quot;内容涵盖&quot;&gt;&lt;/a&gt;内容涵盖&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;分词的正向最大、逆向最大、双向最大匹配法概念；&lt;/li&gt;
&lt;li&gt;词、字符频率统计；（可以使用Python中的c
      
    
    </summary>
    
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
  </entry>
  
  <entry>
    <title>hexo+gitHub 个人博客搭建及更换主题历程--Windows版本（特适合入门小白）</title>
    <link href="http://yoursite.com/2019/04/10/hexo-gitHub/"/>
    <id>http://yoursite.com/2019/04/10/hexo-gitHub/</id>
    <published>2019-04-10T13:55:27.000Z</published>
    <updated>2019-09-13T07:04:21.199Z</updated>
    
    <content type="html"><![CDATA[<h2 id="内容涵盖"><a href="#内容涵盖" class="headerlink" title="内容涵盖"></a>内容涵盖</h2><ul><li><strong>hexo+gitHub 个人博客搭建</strong></li><li><strong>搭建过程中遇到的问题及解决方案</strong>    </li><li><strong>更换主题</strong>  </li></ul><h2 id="1-gitHub-创建博客仓库"><a href="#1-gitHub-创建博客仓库" class="headerlink" title="1. gitHub 创建博客仓库"></a>1. gitHub 创建博客仓库</h2><p><strong>(1) 注册Github（如果已注册可以忽略次此步骤）</strong>   </p><p>&ensp;&ensp;&ensp; 详细注册步骤请参考： <a href="https://blog.csdn.net/NEET007/article/details/51510026" target="_blank" rel="noopener"><strong>GitHub网站注册与登陆</strong></a> </p><p><strong>(2) Github上新建自己的博客项目（即创建仓库）</strong>   </p><p><font color="#DC143C" size="3">&ensp;&ensp;注意：仓库名字要和(1)中注册的Github用户名一致！！！</font>  </p><p>&ensp;&ensp;&ensp;XXX.github.io就是你的博客域名咯~<br><img src="https://github.com/HuanwenW/MyPostImag/blob/master/190410-boke/github-1.jpg?raw=true" alt="chuangjianboke"><span class="img-alt">chuangjianboke</span> </p><h2 id="2-node-和git-安装"><a href="#2-node-和git-安装" class="headerlink" title="2. node 和git 安装"></a>2. node 和git 安装</h2><p><strong>(1) 安装 node.js</strong><br>&ensp;&ensp;&ensp;详细安装步骤请参考：<a href="https://blog.csdn.net/muzidigbig/article/details/80493880" target="_blank" rel="noopener">nodejs详细安装步骤</a><br><strong>(2) 安装 Git</strong><br>&ensp;&ensp;&ensp; 详细注册步骤请参考：<a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000" target="_blank" rel="noopener">廖雪峰老师的git安装教程</a>  </p><h2 id="3-Hexo安装"><a href="#3-Hexo安装" class="headerlink" title="3. Hexo安装"></a>3. Hexo安装</h2><p><strong>(1) 打开cmd命令编辑器</strong>（键盘同时按window+R–输入cmd–回车）    </p><p><strong>(2) 输入全局安装hexo-cli指令</strong>  （下载速度有点慢，耐心~~）  </p><table><tr><td bgcolor="black"> <font size="3" color="white">$ npm  install  -g hexo-cli </font></td></tr></table>  **(3) 初始化hexo**(MyHexo文件夹名字自己取)  &ensp;&ensp;&ensp;某盘下新建一个MyHexo文件夹 --- 单机鼠标右键 --- 选择 Git Bash here-- 出现命令窗口，输入下面命令：    <table><tr><td bgcolor="black"> <font size="3" color="white">$ hexo init blog </font></td></tr></table>  &ensp;&ensp;&ensp;当出现`INFO  Start blogging with Hexo!`表示Hexo初始化安装成功。并且MyHexo文件夹中出现 blog 文件夹。  **(4) 初体验hexo魅力**  &ensp;&ensp;&ensp;由于初始化hexo 之后source目录下自带一篇hello world文章, 所以依次执行下方两个命令：  <font color="#DC143C" size="3">&ensp;注意：执行下面命令前要先进入blog文件夹中重新打开git命令窗口！！！</font>  <table><tr><td bgcolor="black"> <font size="3" color="white">$ hexo generate </font></td></tr></table>   命令含义： 生成静态文件，等价于(可简写为)： `hexo  g`    <table><tr><td bgcolor="black"> <font size="3" color="white">$ hexo server  </font></td></tr></table>   命令含义： 启动本地服务器，等价于(可简写为)： `hexo  s`<p>打开浏览器，输入网址： <a href="http://localhost:4000/" target="_blank" rel="noopener">http://localhost:4000/</a> ，即可看到网站初步的模样。<br><img src="https://github.com/HuanwenW/MyPostImag/blob/master/190410-boke/github-2.png?raw=true" alt="bendibokeliulan"><span class="img-alt">bendibokeliulan</span><br><font color="#8A2BE2" size="3">&ensp;&ensp;&ensp;若出错提示404等，请参考博客4 </font>   </p><h2 id="4-如何将github和hexo联系起来？"><a href="#4-如何将github和hexo联系起来？" class="headerlink" title="4. 如何将github和hexo联系起来？"></a>4. 如何将github和hexo联系起来？</h2><p><strong>(1) 配置SSH key</strong><br> &ensp;&ensp; &ensp;为什么要配置这个呢？因为你提交代码肯定要拥有你的github权限才可以，但是直接使用用户名和密码太不安全了，所以我们使用ssh key来解决本地和服务器的连接问题。<br><strong>a.</strong> 重新打开 Git 命令窗口，输入下面命令：  </p><table><tr><td bgcolor="black"> <font size="3" color="white">$ ssh-keygen -t rsa -C "git@github.com"    //Github的注册邮箱地址 </font></td></tr></table>  **b.** 一路Enter过来就好，得到的信息如下：  <table><tr><td bgcolor="black"> <font size="3" color="white">Your public key has been saved in /c/Users/user/.ssh/id_rsa.pub. </font></td></tr></table>   &ensp;&ensp;&ensp;找到该文件并打开，复制里面的所有内容，然后进入Sign in to GitHub：[https://github.com/settings/ssh](https://github.com/settings/ssh)  <p><strong>c.</strong> 依次执行下面步骤：<br>&ensp;&ensp;&ensp;&ensp;点击 New SSH key —— Title：blog —— Key：输入刚才复制的 —— Add SSH key  </p><p><strong>d.</strong> 测试一下是否成功,输入下方命令：  </p><table><tr><td bgcolor="black"> <font size="3" color="white">$ ssh -T git@github.com   //Github的注册邮箱地址 </font></td></tr></table>  ![SSH](https://github.com/HuanwenW/MyPostImag/blob/master/190410-boke/github-3.png?raw=true)  &ensp;&ensp;&ensp;看到上面信息说明SSH已配置成功！  <p><strong>(2) 设置Git的user name和email</strong><br> &ensp;&ensp;&ensp;blog文件夹下的Git命令窗口依次执行下方两个命令：  </p><table><tr><td bgcolor="black"> <font size="3" color="white"> $ git config --global user.name "liuxianan"   // 你的github用户名，非昵称 </font></td></tr></table>  <table><tr><td bgcolor="black"> <font size="3" color="white">$ git config --global user.email  "git@github.com"   // 填写你的github注册邮箱 </font></td></tr></table>&ensp;&ensp;&ensp;设置这个是为了便与之后上传到github的page上。  <p><strong>(3) 修改参数及配置deployment</strong><br>&ensp;&ensp;&ensp;在blog目录下，打开_config.yml文件。<br><strong>a.</strong> 修改网站相关参数信息<br><font color="#DC143C" size="3">&ensp;注意：在每个参数的：后都要加一个空格！！！</font>      </p><pre><code>title: 焕小妹博客               //博客名字subtitle: bug                  //博客副标题description: Bug多多，欢乐多多  //博客描述author: 焕焕                   //作者language: zh-CN               //zh-CN 表中文timezone: Asia/Shanghai       //时间</code></pre><p><strong>b.</strong> 设置deployment参数信息<br><font color="#DC143C" size="3">&ensp;注意：仓库地址后加 .git  结尾！！！</font>   </p><pre><code>deploy:type: gitrepository: git@github.com:saucxs/saucxs.github.io.git   //Github注册邮箱：Github用户名/Github用户名.github.io.gitbranch: master</code></pre><p><strong>c.</strong> 发布到网上,执行下面命令：</p><table><tr><td bgcolor="black"> <font size="3" color="white">$ hexo deploy  </font></td></tr></table>    命令含义： 部署，等价于(可简写为)： `hexo  d`   <font color="#8A2BE2" size="3">&ensp;&ensp;&ensp;若出错提示找不到git，请参考博客5 </font>  **d.** 测试是否发布成功  &ensp;&ensp;&ensp;打开浏览器，输入网址： https://你的Github名.github.io ，即可看到部署好的网站，别人也可以访问。    ## 5. 主题不喜欢，换！  hexo默认的主题是landscape，可以根据自己的喜好，换你喜欢的主题。  **(1) 进入github官方主题界面，选择你喜欢的主题！**  &ensp;&ensp;&ensp;Hexo主题网站地址：[https://hexo.io/themes/](https://hexo.io/themes/)  **(2) 复制主题地址**  &ensp;&ensp;&ensp;假如你喜欢的主题是：Anatole   **a.** 点击进入主题拥有者的博客：  ![zhutirukou](https://github.com/HuanwenW/MyPostImag/blob/master/190410-boke/github-4.jpg?raw=true)  **b.** 找到博主仓库入口进入(一般会有github图标)  **c.** 搜索栏中搜索刚才那个主题的名字     ![zhuti](https://github.com/HuanwenW/MyPostImag/blob/master/190410-boke/github-5.jpg?raw=true)  **d.** 点进去这篇博客，复制下载地址     ![address](https://github.com/HuanwenW/MyPostImag/blob/master/190410-boke/github-6.jpg?raw=true)  **(3) 下载主题**   &ensp;&ensp;&ensp;打开本地博客文件夹(这里对应上文的blog文件夹) —— 打开git命令窗口 —— 输入下面命令：  <font color="#DC143C" size="3">&ensp;注意：仓库地址后加 .themes/Anatole  结尾！！！</font>   <table><tr><td bgcolor="black"> <font size="3" color="white">$ git clone https://github.com/gaussic/hexo-theme-Anatole.git.themes/Anatole  </font></td></tr></table>    &ensp;&ensp;&ensp;下载完之后会在themes 目录下生成一个名为Anatole文件  **(4) 更改配置文件**  &ensp;&ensp;&ensp;更改一下blog工程(文件夹)目录下的配置文件_config.yml，主题名修改一下即可  ![theme](https://github.com/HuanwenW/MyPostImag/blob/master/190410-boke/github-7.jpg?raw=true)  **(5) 更新并发布新主题**&ensp;&ensp;&ensp;依次执行下方命令：  (每一步作用及执行后界面，可参考上文)  <table><tr><td bgcolor="black"> <font size="3" color="white">$ hexo clean </font></td></tr></table>  <table><tr><td bgcolor="black"> <font size="3" color="white">$ hexo g  </font></td></tr></table>  <table><tr><td bgcolor="black"> <font size="3" color="white">$ hexo s  </font></td></tr></table>  <table><tr><td bgcolor="black"> <font size="3" color="white">$ hexo d  </font></td></tr></table>  **(6) 测试新主题**  &ensp;&ensp;&ensp;打开浏览器，输入网址： https://你的Github名.github.io ，即可看到更新后的主题网站。  ## 6. 创建新的博客及上传！  **(1) 创建新的博客**     &ensp;&ensp;&ensp;打开git命令窗口，输入下方命令：<table><tr><td bgcolor="black"> <font size="3" color="white">$ hexo new '博客文章名字' </font></td></tr></table>  &ensp;&ensp;&ensp;在目录..\blog\source\_posts下即可看到新建的.md文件  **(2) 遵循Markdown语法书写博客**  <font color="#DC143C" size="3">&ensp;友情链接：</font>**a.** [Markdowen介绍及语法](https://www.jianshu.com/p/7771794c88a1)&ensp;&ensp;&ensp;**b.** [MarkdownPad2安装教程](https://www.jianshu.com/p/5604996dcdbb)  <p><strong>(3) 提交博客到GitHub仓库</strong><br>&ensp;&ensp;&ensp;打开本地博客文件夹(这里对应上文的blog文件夹) —— 打开git命令窗口 —— 输入以下命令：<br><strong>a.</strong> 清空:</p><table><tr><td bgcolor="black"> <font size="3" color="white">$  hexo clean </font></td></tr></table>  **b.** 生成静态文件:<table><tr><td bgcolor="black"> <font size="3" color="white">$  hexo g </font></td></tr></table>   **c.** 将本地运行:<table><tr><td bgcolor="black"> <font size="3" color="white">$  hexo s </font></td></tr></table>   **d.** 发布到github的page上 <table><tr><td bgcolor="black"> <font size="3" color="white">$  hexo d </font></td></tr></table>  **e.** 查看更新   &ensp;&ensp;&ensp;打开浏览器，输入网址： https://你的Github名.github.io ，即可看到更新的博客。<h2 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h2><ol><li><a href="https://zhangslob.github.io/2017/02/28/%E6%95%99%E4%BD%A0%E5%85%8D%E8%B4%B9%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%EF%BC%8CHexo-Github/" target="_blank" rel="noopener"><strong>教你免费搭建个人博客，Hexo&amp;Github</strong></a></li><li><a href="https://www.cnblogs.com/chengxs/p/7430283.html" target="_blank" rel="noopener">github+hexo搭建自己的博客网站（二）<strong>更换主题yilia</strong></a></li><li><a href="https://blog.csdn.net/weixin_42419856/article/details/81141546" target="_blank" rel="noopener"><strong>如何更改使用hexo-github搭建博客的主题 theme</strong></a></li><li><a href="https://blog.csdn.net/qq32933432/article/details/87955133" target="_blank" rel="noopener"><strong>hexo搭建Github博客上传后，访问网页显示404问题解决方案</strong></a></li><li><a href="https://blog.csdn.net/weixin_36401046/article/details/52940313" target="_blank" rel="noopener"><strong>hexo d后 ERROR Deployer not found: git</strong></a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;内容涵盖&quot;&gt;&lt;a href=&quot;#内容涵盖&quot; class=&quot;headerlink&quot; title=&quot;内容涵盖&quot;&gt;&lt;/a&gt;内容涵盖&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;hexo+gitHub 个人博客搭建&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;搭建过
      
    
    </summary>
    
    
      <category term="技术篇" scheme="http://yoursite.com/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="软件安装" scheme="http://yoursite.com/tags/%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85/"/>
    
  </entry>
  
  <entry>
    <title>Markdown 插入图片不友好，如何解决？</title>
    <link href="http://yoursite.com/2019/04/09/Markdown-Picture/"/>
    <id>http://yoursite.com/2019/04/09/Markdown-Picture/</id>
    <published>2019-04-09T13:55:27.000Z</published>
    <updated>2019-04-13T11:08:40.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前奏："><a href="#前奏：" class="headerlink" title="前奏："></a>前奏：</h3><p>&ensp;&ensp;&ensp;想要Markdown中图文并茂记录学习历程，偏偏却卡在了插图上，经过牛牛二虎之力，灰心丧气，终于得救<del>~</del>下面我来综合多篇博客，给入门的小白(git使用都包教)一条生路！</p><ul><li><strong>环境</strong>：基于github网页版Markdown图片插入记录  </li><li><strong>问题来源：</strong> <a href="https://blog.csdn.net/slaughterdevil/article/details/79255933" target="_blank" rel="noopener">MarkDown添加图片的三种方式</a>无法满足我的需求，问题如下：<br><img src="https://github.com/HuanwenW/MyPostImag/blob/master/007-picture/markD-%E6%8F%92%E5%9B%BE-1.jpg?raw=true" alt="image"><span class="img-alt">image</span>    </li><li><strong>解决问题思路：</strong>  （截图中提到的方法1和2结合）即本地图片生成链接（保证图片何时何地都可以显示）+ 插入图片连接方法  </li><li><strong>前方高能，提高注意力咯！！</strong>  </li></ul><h2 id="1-GitHub-建立仓库篇"><a href="#1-GitHub-建立仓库篇" class="headerlink" title="1. GitHub 建立仓库篇"></a>1. GitHub 建立仓库篇</h2><p><strong>第一步：</strong> 在GitHub上建立一个图片存储仓库<br>(1) 比如我建立的MyPostImage，用来存储需要的图片。<br><img src="https://github.com/HuanwenW/MyPostImag/blob/master/190406-markDown/MD-Pic-1.png?raw=true" alt="image"><span class="img-alt">image</span><br>(2) 复制项目地址：<br><img src="https://github.com/HuanwenW/MyPostImag/blob/master/190406-markDown/MD-Pic-2.jpg?raw=true" alt="image"><span class="img-alt">image</span><br><strong>第二步：</strong> 把Github项目-MyPostImage克隆到本地 ：<br>(1) 新建一个文件夹-githubPicture，在<strong>文件夹githubPicture内</strong>点击右键–选择Git Bash Here 打开终端，输入命令：<br><img src="https://github.com/HuanwenW/MyPostImag/blob/master/190406-markDown/MD-Pic-3.png?raw=true" alt="image"><span class="img-alt">image</span><br>(2) 克隆完成后结果如下：<br><img src="https://github.com/HuanwenW/MyPostImag/blob/master/190406-markDown/MD-Pic-4.png?raw=true" alt="image"><span class="img-alt">image</span><br>(3) 发现原来的文件中出现 <strong>MyPostImag</strong>文件夹<br><img src="https://github.com/HuanwenW/MyPostImag/blob/master/190406-markDown/MD-Pic-5.jpg?raw=true" alt="image"><span class="img-alt">image</span><br><strong>第三步：</strong> 在克隆到本地的文件夹中建立一个文件夹–<strong>190406-markDown</strong>(名字自己取) ：<br><img src="https://github.com/HuanwenW/MyPostImag/blob/master/190406-markDown/MD-Pic-6.jpg?raw=true" alt="image"><span class="img-alt">image</span><br><strong>第四步：</strong> 把你要用的图片存到文件夹<strong>190406-markDown</strong>里。<br><strong>第五步：</strong> 把更改push到Github仓库：<br> （1） 输入 “ git init “，作用是项目里面会创建一个隐藏的.git文件，执行结果如下：<br> <img src="https://github.com/HuanwenW/MyPostImag/blob/master/190406-markDown/MD-Pic-7.jpg?raw=true" alt="image"><span class="img-alt">image</span><br> （2） 输入 “ git add . “, 这个是将项目上所有的文件添加到仓库中的意思，如果想添加某个特定的文件，只需把’ . ‘换成这个特定的文件名即可。<br> <img src="https://github.com/HuanwenW/MyPostImag/blob/master/190406-markDown/MD-Pic-8.png?raw=true" alt="image"><span class="img-alt">image</span><br> （3） 输入 “ git commit -m “first commit “，表示你对这次提交的注释，双引号里面的内容可以根据个人的需要改。例如 “ git commit -m “第一次提交 “<br> <img src="https://github.com/HuanwenW/MyPostImag/blob/master/190406-markDown/MD-Pic-9.png?raw=true" alt="image"><span class="img-alt">image</span><br> （4） 输入 “git remote add origin https://刚才建立的仓库地址（第一步中提到的）”  将本地的仓库关联到github上。<br> <img src="https://github.com/HuanwenW/MyPostImag/blob/master/190406-markDown/MD-Pic-10.png?raw=true" alt="image"><span class="img-alt">image</span><br> （5） 输入 “git push -u origin master “，这是把代码上传到github仓库的意思。<br> <img src="https://github.com/HuanwenW/MyPostImag/blob/master/190406-markDown/MD-Pic-11.png?raw=true" alt="image"><span class="img-alt">image</span><br> （6） 刷新github页面，发现本地新建文件已经出现：<br> <img src="https://github.com/HuanwenW/MyPostImag/blob/master/190406-markDown/MD-Pic-12.png?raw=true" alt="image"><span class="img-alt">image</span>   </p><h2 id="2-图片生成URL篇"><a href="#2-图片生成URL篇" class="headerlink" title="2. 图片生成URL篇"></a>2. 图片生成URL篇</h2><p>（1）打开 <strong>190406-markDown</strong> 文件夹，双击你需要的图片进入到下面界面：<br><img src="https://github.com/HuanwenW/MyPostImag/blob/master/190406-markDown/MD-Pic-13.png?raw=true" alt="image"><span class="img-alt">image</span><br>（2）右键选择<strong>复制图片地址</strong>，然后按照 ![图片名称](复制好的图片地址） 格式，添加到Markdown博文中即可，比如：<br>我下面插入的图3的图片是这样实现的:  </p><blockquote><p>![MD-Pic-13.png]（<a href="https://github.com/HuanwenW/MyPostImag/blob/master/190406-markDown/MD-Pic-13.png?raw=true）" target="_blank" rel="noopener">https://github.com/HuanwenW/MyPostImag/blob/master/190406-markDown/MD-Pic-13.png?raw=true）</a>  </p></blockquote><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ol><li><a href="https://www.jianshu.com/p/e9f18be1295d" target="_blank" rel="noopener">简单三步在Markdown 中插入图片</a></li><li><a href="https://blog.csdn.net/hustwayne/article/details/83014499" target="_blank" rel="noopener">Github项目（克隆，上传）简单git命令流程使用记录</a></li><li><a href="https://blog.csdn.net/linton1/article/details/80320121" target="_blank" rel="noopener">GitHub上克隆项目到本地</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;前奏：&quot;&gt;&lt;a href=&quot;#前奏：&quot; class=&quot;headerlink&quot; title=&quot;前奏：&quot;&gt;&lt;/a&gt;前奏：&lt;/h3&gt;&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;想要Markdown中图文并茂记录学习历程，偏偏却卡在了插图上，经过牛牛二虎之力，灰心丧气，终于
      
    
    </summary>
    
    
      <category term="技术篇" scheme="http://yoursite.com/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="软件安装" scheme="http://yoursite.com/tags/%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85/"/>
    
  </entry>
  
</feed>
